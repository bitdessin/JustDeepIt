from agrolens.models.utils.u2net import U2Net


class SOD():
    """Base module to generate salient object detection model
    
    Class :class:`SOD <agrolens.models.SOD>` generates U\ :sup:`2`-Net for salient object detection model.
 

    Args:
        model_arch (str): A string to specify model architecture.
                          The current AgroLens only supports U\ :sup:`2`-Net architecture.
        model_weight (str): A path to a model weights. If ``None``, then use the initial
                        value that randomly generated by the packages.
        workspace (str): A path to workspace directory. Log information and checkpoints of model
                     training will be stored in this directory.
                     If ``None``, then create temporary directory in system temporary directory
                     such as :file:`/tmp` and will be removed after finishing the program.
 
    Examples:
            >>> from agrolens.models import SOD
            >>> 
            >>> # initialize U2-Net with random weight
            >>> model = SOD()
            >>> 
            >>> # initialize U2-Net with trained weight (./leaf.u2net.pth)
            >>> model = SOD(model_weight='./leaf.u2net.pth')
    
    """
    
    
    def __init__(self, model_arch='u2net', model_weight=None, workspace=None):
        self.module = U2Net(model_weight, workspace)
    
    
    def train(self, train_data_fpath, batch_size=32, epoch=10000, cpu=8, gpu=1,
              strategy='resize', window_size=320):
        """Train model
        
        Method :func:`train <agrolens.models.SOD.train>` is used for
        training a model using *resizing* or *random cropping* approach.
        The *resizing* approach scales the original image to 288 × 288 pixels for training,
        whereas *random cropping* randomly crops small square areas from the original image,
        and resizes the areas to 288 × 288 pixels for training.
        The size of the square areas can be specified by the user
        according to the characteristics of the target task.
        The default size of the cropped square area is 320 x 320 pixels.
        
        Args:
            train_data_fpath (str): A path to the tab-separeted file which contains the two columns.
                                    On each line, the first column records a path to a training image,
                                    and the second column records a path to the corresponding mask image.
            batch_size (int): Number of batch size.
                              Note that a large number of batch size may cause out of memory error.
            epoch (int): Number of epochs.
            cpu (int): Number of CPUs are used for prerpocessing training images.
            gpu (int): Number of GPUs are uesd for traininng model.
            strategy (str): Strategy for model trainig. One of ``resize`` or ``randomcrop`` can be specified.
            window_size (int): The width of images should be randomly cropped from the original images
                                    when ``randomcrop`` srategy was selected.


        Examples:
            >>> from agrolens.models import SOD
            >>> 
            >>> model = SOD()
            >>> 
            >>> # train_images.txt --------------------
            >>> # train_image_01.jpg  train_mask_01.png
            >>> # train_image_02.jpg  train_mask_02.png
            >>> # train_image_03.jpg  train_mask_03.png
            >>> # train_image_04.jpg  train_mask_04.png
            >>> #         :                   :
            >>> # -------------------------------------
            >>> 
            >>> model.train('train_images.txt')
            >>> 
        
        """
        
        return self.module.train(train_data_fpath, batch_size, epoch, cpu, gpu,
                          strategy, window_size)


   
    
    def save(self, weight_fpath):
        """Save weights
        
        Method to store the current weights.

        Args:
            weight_fpath (str): A path to store model weights.

        Examples:
            >>> from agrolens.models import SOD
            >>> 
            >>> model = SOD()
            >>> model.train('train_images.txt')
            >>> model.save('final_weight.pth')
            >>> 

        """
        
        return self.module.save(weight_fpath)





    
    
    def inference(self, image_path, strategy='resize', batch_size=8, cpu=4, gpu=1,
                  window_size=320,
                  u_cutoff=0.1, image_opening_kernel=0, image_closing_kernel=0):
        """Object Segmentation
        
        Method :func:`inference <agrolens.models.SOD.inference>` performs
        salient object detection through *resizing* or *sliding* approach.
        The *resizing* approach resizes the original image to 288 × 288 pixels for model training.
        On the other hand, *sliding* crops adjacent square areas from the original image
        for input to the model,
        and the outputs are merged into a single image.
        The size of the square areas can be specified by the user,
        but it is recommended to use the value specified by ``window_size`` for training.
        
        Args:
            image_path (str): A path to a image file, a list of image files,
                              or a path to a directory which contains multiple images.
            strategy (str): Strategy for model trainig. One of ``resize`` or ``slide`` can be specified.
            output_type (str): Output format. 
            batch_size (int): Number of batch size. Note that a large number of
                              batch size may cause out of memory error.
            epoch (int): Number of epochs.
            cpu (int): Number of CPUs are used for prerpocessing training images.
            gpu (int): Number of GPUs are used for object segmentation.
            window_size (int): The width of images should be cropped from the original images
                                       when ``slide`` srategy was selected.
            u_cutoff (float): A threshold to cutoff U2Net outputs. Values higher than this threshold
                              are considering as detected objects.
            image_opening_kernel (int): The kernel size for image closing
                                        to remove the noise that detected as object.
            image_closing_kernel (int): The kernel size for image opening
                                        to remove the small bubbles in object.

        Returns:
            array: mask image or mask annotations.
        
        Examples:
            >>> import os
            >>> from agrolens.models import SOD
            >>> 
            >>> model = SOD(model_weight='trained_weight.pth')
            >>> 
            >>> # single image
            >>> output = model.inference('sample.jpg')
            >>> output.draw('mask', 'sample.predicted_mask.png')
            >>> 
            >>> # multiple images
            >>> test_images = ['sample1.jpg', 'sample2.jpg', 'sample3.jpg']
            >>> outputs = model.inference(test_images)
            >>> for test_image, output in zip(test_images, outputs):
            >>>     mask_fpath = os.path.splitext(test_image) + '_mask.png'
            >>>     output.draw('mask', mask_fpath)
            
        """
        
        return self.module.inference(image_path, strategy, batch_size, cpu, gpu,
                              window_size, u_cutoff,
                              image_opening_kernel, image_closing_kernel)




