import os
import glob
from justdeepit.models.utils.u2net import U2Net
from justdeepit.utils import ImageAnnotation, ImageAnnotations

class SOD():
    """Base module to generate salient object detection model
    
    The :class:`SOD <justdeepit.models.SOD>` class is used for generating
    U\ :sup:`2`-Net for salient object detection.
 

    Args:
        model_arch (str): A string to specify model architecture.
                          The current version of JustDeepIt only supports U\ :sup:`2`-Net architecture.
        model_weight (str): A path to a model weights. If ``None``, then use the initial
                            value that randomly generated by the packages.
        workspace (str): A path to workspace directory. Log information and checkpoints of model
                     training will be stored in this directory.
                     If ``None``, then create temporary directory in the current directory.
 
    Examples:
            >>> from justdeepit.models import SOD
            >>> 
            >>> # initialize U2-Net with random weight
            >>> model = SOD()
            >>> 
            >>> # initialize U2-Net with trained weight (./leaf.u2net.pth)
            >>> model = SOD(model_weight='./leaf.u2net.pth')
    
    """
    
    
    def __init__(self, model_arch='u2net', model_weight=None, workspace=None):
        if workspace is None:
            workspace = os.path.abspath(os.getcwd())
        self.workspace = workspace
        
        self.__architectures = ('U2Net',)
        self.__supported_formats = ('mask', 'COCO')
        self.__image_ext = ['.jpg', '.jpeg', '.png', '.tif', '.tiff']
        self.module = self.__init_module(model_arch, model_weight, workspace)
    
    
    def __init_module(self, model_arch, model_weight, workspace):
        if model_arch is None:
            return None
            
        model_arch = model_arch.replace('-', '').replace(' ', '').lower()
        if model_arch == 'u2net':
            module = U2Net(model_weight, workspace)
        else:
            NotImplementedError('JustDeepIt does not support {}.')

        return module
        


    def available_architectures(self, backend):
        """Show the available architectures

        Show the available architectures for salient object detection.

        Returns:
            A tuple of the supported architecture.

        Examples:
            >>> from justdeepit.models import SOD
            >>>
            >>> model = SOD()
            >>> model.available_architectures()

        """
        return self.__architectures



    def supported_formats(self):
        """Show the supported annotation formats

        Show the supported annotation formats for salient object detection.

        Returns:
            A tuple of the supported annotation formats.

        Examples:
            >>> from justdeepit.models import SOD
            >>>
            >>> model = SOD()
            >>> model.supported_formats()

        """
        return self.__supported_formats

    
    def __norm_format(self, x):
        x = x.replace(' ', '').replace('-', '').lower()
        if ('pascal' in x) or ('xml' in x):
            x = 'voc'
        return x
    
    

    def train(self, image_dpath, annotation, annotation_format='mask',
              optimizer=None, scheduler=None,
              batchsize=8, epoch=100, cpu=4, gpu=1,
              strategy='resizing', window_size=320):
        """Train model
        
        Method :func:`train <justdeepit.models.SOD.train>` is used for
        training a model using *resizing* or *random cropping* approach.
        The *resizing* approach scales the original image to 288 × 288 pixels for training,
        whereas *random cropping* randomly crops small square areas from the original image,
        and resizes the areas to 288 × 288 pixels for training.
        The size of the square areas can be specified by the user
        according to the characteristics of the target task.
        The default size of the cropped square area is 320 x 320 pixels.
        
        Args:
            image_dpath (str): A path to directory which contains all training images.
            annotation (str): A path to a file (when the format is COCO) or folder (when the format is mask).
            annotation_format (str): Annotation format.
            optimizer (str): String to specify PyTorch optimizer. The optimizers supported by
                    PyTorch can be checked from
                    the `PyTorch website <https://pytorch.org/docs/stable/optim.html>`_.
            scheduler (str): String to specify PyTorch optimization scheduler. The schedulers supported by
                    PyTorch can be checked from
                    the `PyTorch website <https://pytorch.org/docs/stable/optim.html>`_.
            batchsize (int): Number of batch size.
                             Note that a large number of batch size may cause out of memory error.
            epoch (int): Number of epochs.
            cpu (int): Number of CPUs are used for prerpocessing training images.
            gpu (int): Number of GPUs are uesd for traininng model.
            strategy (str): Strategy for model trainig. One of ``resizing`` or ``randomcrop`` can be specified.
            window_size (int): The width of images should be randomly cropped from the original images
                                    when ``randomcrop`` srategy was selected.


        Examples:
            >>> from justdeepit.models import SOD
            >>> 
            >>> model = SOD()
            >>> 
            >>> model.train('./train_images', './mask_images', 'mask')
            >>> 
            >>> 
            >>> # set optimizer and scheduler
            >>> model.train('./train_images', '.mask_images', 'mask',
            >>>             optimizer='Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)',
            >>>             scheduler='ExponentialLR(optimizer, gamma=0.9)')
            >>> 
        """
        
        images = []
        masks = []
        annotation_format = self.__norm_format(annotation_format)
        if annotation_format == 'mask':
            fdict = {}
            for f in glob.glob(os.path.join(image_dpath, '*')):
                fname = os.path.splitext(os.path.basename(f))[0]
                if os.path.splitext(f)[1].lower() in self.__image_ext:
                    if fname not in fdict:
                        fdict[fname] = {'image': None, 'mask': None}
                    fdict[fname]['image'] = f
            for f in glob.glob(os.path.join(annotation, '*')):
                fname = os.path.splitext(os.path.basename(f))[0]
                if fname not in fdict:
                    fdict[fname] = {'image': None, 'mask': None}
                fdict[fname]['mask'] = f
            for fname, fpath in fdict.items():
                if fpath['image'] is not None and fpath['mask'] is not None:
                    images.append(fpath['image'])
                    masks.append(fpath['mask'])
           
        elif annotation_format == 'coco':
            # generate mask for mdoel training form coco annotation
            mask_dpath = os.path.join(self.workspace, 'mask')
            if not os.path.exists(mask_dpath):
                os.mkdir(mask_dpath)
            
            for f in sorted(glob.glob(os.path.join(image_dpath, '*'))):
                image_fname, image_fext = os.path.splitext(f)
                if image_fext.lower() in self.__image_ext:
                    ann = ImageAnnotation(f, annotation, annotation_format)
                    m = os.path.join(mask_dpath, os.path.splitext(os.path.basename(image_fname))[0] + '.mask.png')
                    ann.draw('mask', m)
                    images.append(f)
                    masks.append(m)
        else:
            raise NotImplementedError('JustDeepIt does not support {} format for training salient object detection model.'.format(annotation_format))
        
        
        train_data_fpath = os.path.join(self.workspace, 'train_images.txt')
        with open(train_data_fpath, 'w') as outfh:
            for image, mask in zip(images, masks):
                outfh.write('{}\t{}\n'.format(image, mask))
        
        return self.module.train(train_data_fpath,
                          optimizer, scheduler,
                          batchsize, epoch,
                          cpu, gpu,
                          strategy, window_size)


   
    
    def save(self, weight_fpath):
        """Save weights
        
        Method to store the current weights.

        Args:
            weight_fpath (str): A path to store model weights.

        Examples:
            >>> from justdeepit.models import SOD
            >>> 
            >>> model = SOD()
            >>> model.train('./train_images', './mask_images', 'mask')
            >>> model.save('trained_weight.pth')
            >>> 

        """
        
        return self.module.save(weight_fpath)





    
    
    def inference(self, image_path, strategy='resizing', batchsize=8, cpu=4, gpu=1,
                  window_size=320,
                  score_cutoff=0.5, image_opening_kernel=0, image_closing_kernel=0):
        """Object Segmentation
        
        Method :func:`inference <justdeepit.models.SOD.inference>` performs
        salient object detection through *resizing* or *sliding* approach.
        The *resizing* approach resizes the original image to 288 × 288 pixels for model training.
        On the other hand, *sliding* crops adjacent square areas from the original image
        for input to the model,
        and the outputs are merged into a single image.
        The size of the square areas can be specified by the user,
        but it is recommended to use the value specified by ``window_size`` for training.
        
        Args:
            image_path (str): A path to a image file, a list of image files,
                              or a path to a directory which contains multiple images.
            strategy (str): Strategy for model trainig. One of ``resizing`` or ``slide`` can be specified.
            output_type (str): Output format. 
            batchsize (int): Number of batch size. Note that a large number of
                              batch size may cause out of memory error.
            epoch (int): Number of epochs.
            cpu (int): Number of CPUs are used for prerpocessing training images.
            gpu (int): Number of GPUs are used for object segmentation.
            window_size (int): The width of images should be cropped from the original images
                                       when ``slide`` srategy was selected.
            score_cutoff (float): A threshold to cutoff U2Net outputs. Values higher than this threshold
                              are considering as detected objects.
            image_opening_kernel (int): The kernel size for image closing
                                        to remove the noise that detected as object.
            image_closing_kernel (int): The kernel size for image opening
                                        to remove the small bubbles in object.

        Returns:
            array: mask image or mask annotations.
        
        Examples:
            >>> import os
            >>> from justdeepit.models import SOD
            >>> 
            >>> model = SOD(model_weight='trained_weight.pth')
            >>> 
            >>> # single image
            >>> output = model.inference('sample.jpg')
            >>> output.draw('mask', 'sample.predicted_mask.png')
            >>> 
            >>> # multiple images
            >>> test_images = ['sample1.jpg', 'sample2.jpg', 'sample3.jpg']
            >>> outputs = model.inference(test_images)
            >>> for test_image, output in zip(test_images, outputs):
            >>>     mask_fpath = os.path.splitext(test_image) + '_mask.png'
            >>>     output.draw('mask', mask_fpath)
            
        """
        
        return self.module.inference(image_path, strategy, batchsize, cpu, gpu,
                              window_size, score_cutoff,
                              image_opening_kernel, image_closing_kernel)




